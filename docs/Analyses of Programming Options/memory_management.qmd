---
title: Memory Management
date: last-modified
date-format: DD MMMM YYYY
execute:
  debug: true
toc: true
toc-depth: 4
number-sections: false
bibliography: bibliography.bib
format:
  html:
    embed-resources: true
  pdf:
    documentclass: report
jupyter: python3
editor: visual
---

# Memory Management

Both Python and R were based on C, but have lost C's ability to manually allocate or control memory.

## General Concepts:

### Threads:

A thread is a sequential execution stream, and a *process* can be composed of one or many threads. Being a sequential execution stream, threads must execute their tasks in a sequential order, one instruction at a time [@stanf_threads].

A process (program) is composed of it's thread(s) and a specific *execution state.* The execution state may include any code, data, call stacks, open files, network connections, register/memory allocation, or anything else that may affect, or be affected by, a thread. In a single-threaded process, the entire process is contained within a single thread. In a multi-threaded process, there may be elements of the *execution state* which are shared between threads, and elements which are private to each thread [@stanf_threads].

The earliest computing systems were all single-threaded. Some computing advancements allowed for the implementation of *Monitors,* which could manage concurrent single-threaded programs by allocating the program's access to the CPU thread, blocking other programs during that time. This wasn't true concurrency, but facilitated more complex computing tasks by maximizing the CPU's throughput [@armsearlytimesharing]. In practice, this means that at any given time, a thread is either *running*, *blocked* and waiting for an event (such as a network packet or disk I/O access), or *ready* and waiting for CPU time [@stanf_threads].

The concept of a Monitor evolved in complexity into the modern day concept of an *Operating System* (OS) [@oshistory].

### Multithreading:

Tasks with a large number of intake/output operations (IO bound) can take advantage of multithreading. Multithreading is primarily limited by CPU throughput (clock speed).

In multithreading, tasks within a process are divided into threads within a shared memory space which can execute instructions *simultaneously* and *independently.* This is what allows parallel processing in multi-threading to function: each thread is performing processing *independently* and *simultaneously* [@stanf_threads].

Even if not processed simultaneously, multi-threading has many advantages for complex computations. Threads of a singular process share much of their *execution state*, which means that each thread individually only requires additional memory allocation for it's private portion of the *execution state.* This additional memory allocation is relatively quite small in comparison to an additional process. Additionally, the *Operating System* is able to optimise the CPU throughput of these individual threads, even if the CPU can only process a single thread at one time. [@ibm_threads].

The possibility of simultaneous processing lead to a need to solve the *race condition*:

-   If two threads are attempting to update the same variable in the shared memory space at the same time, what happens?
-   If one thread is attempting to read a variable at the same time another thread is attempting to update that variable, what happens?
-   If a thread gets interrupted, what happens?

Solutions to the race condition can depend on the CPU's ability to handle multithreading, a system's ability for multiprocessing, and the constraints of a programming language [@kstate_racecond].

Historically, CPUs could only process one thread at any given moment. This meant that the race condition was solved through timing: only one thread was processed by the CPU at a time. Any computationally-heavy tasks, which are CPU-bound, would have an upper-boundary of efficiency set by the CPU's throughput. In order to increase CPU throughput, one would have to increase clock speed. Increases to clock speed ran the risk of increased heat production and damage to hardware components. Multiprocessing helps address these limitations [@intel_clockspd].

### Multiprocessing:

CPU-bound, computationally-heavy tasks benefit most from multiprocessing. Multiprocessing may be memory-heavy in execution.

Multiprocessing involves the support of multiple processors and the ability to allocate processes between them. This increases the complexity of the computing system. Each processor is similar to having multiple identical copies of a computing system [@stanf_threads].

Because multiprocessing cannot allocate threads between procesors, and instead allocates processes, multiprocessing can be a memory heavy task. Each processor running a copy of the process will have to allocate memory for the execution state of the *entire process,* rather than just a thread. There are some mechanisms which may be used to facilitate communication between these parallel processes, but these mechanisms may be limited by a programming language [@ibm_threads].

Modern CPU chips are actually microprocessors which contain multiple cores, each of which is functionally a complete CPU and can execute a thread. Many of these modern processors also support *hyperthreading,* where each physical core behaves as if it were actually multiple cores [@stanf_threads]. For example, the Intel Core i7-11370H Processor has 4 physical cores (Quad-Core), but each core supports up to two threads (2-way hyperthreading), for a practical total of 8 threads [@inteli7]. These modern microprocessor CPUs facilitate both multithreading and multiprocessing.

## Python:

### A Note on Variables and References

Python's variables do not directly hold values. Rather, variables are directed towards a region of the memory space which holds the value. In this way, variables function as "shortcuts" and "address books" rather than actual containers. This means variables are not bound to any single object type.

Python has both mutable and immutable data types. Python always maps identical immutable data types to the same Object in memory.

```{python}
a = 12
b = 12
print(a==b)
print(a is b)
```

It might be tempting to think that `is` and `==` are synonymous, but this is not true. The logical operator `is` compares the "memory addresses" to see if the variables reference the same area of memory space. The logical operator `==`, in contrast, determines if the values referenced by the variables are equal, even if they are in a different part of memory space. This is particularly important for mutable data types, like lists.

```{python}
a = [1,2,3]
b = [1,2,3]
print(a==b)
print(a is b)
```

When a variable is assigned to another variable, it "copies" the memory address of that variable

```{python}
a = 12
b = a
print(a==b)
print(a is b)
```

This can lead to unexpected events when mutable data types are altered.

```{python}
a = ["a","b","c","d"]
b = a
a.append("adding this to a!")
print(b)
```

These same issues do not occur with immutable data types, as changing an immutable data type requires a new memory reference.

```{python}
a = 13
b = a
a = 19
print(b)
```

There is a global function `del`, which can delete a variable and the contained "memory address" within it [@python3ref, "Data Structures"]

```{python}
a = 10
print(a)
del a
try:
    print(a)
except Exception as e:
    print(e)
```

### General Principles

The Python 3 Documentation \[\@python3ref\], specifically theÂ "Memory Management" section, outlines much of Python's memory management and principles.

Python has many interpreters, but the reference interpreter CPython is most commonly used; essentially, CPython is a C program that takes the Python code and interprets it based on the rules of Python.

There are a few common elements to any Python program:

-   Methods: all functions, operations, and instructions

-   Objects: all objects used by the program

-   Register: "memory addresses" to all objects.

When a Python program is executed, it passes through the Python interpreter and generates elements of an *execution state* that require memory to function:

-   Code: the interpreted instructions that the computer understands, and will follow-line by-line, step-by-step

-   Heap: the dynamic memory which stores objects

-   Stack: the static, temporary form of memory where actual processing occurs; functions and variables are loaded in from the *Heap* to the *Stack* based on the instructions in the *Code*, return their output to the *Heap* based on the *Code*, and are then removed to free memory

While *Stack* memory must be on the RAM at all times, the *Heap* may include both disc memory and RAM.

CPython operates with a *Global Interpreter Lock* (GIL). This GIL is like a "baton" -- if a resource wishes to access the same Python object bytecode, it must wait for that "baton" to be available. For many programs, this may lock them into single threading, which blocks the parallelism of a multi-processor machine. The GIL is always released when doing I/O tasks, and there are extension modules designed to carefully release the GIL for computationally heavy tasks that benefit from multi-threading, such as compression and hashing.

The Python Interpreter contains a Python Memory Manager, which interacts directly with the OS Memory Manager using C to allocate a Private Heap of Python-exclusive memory. This Private Heap of memory is exclusively allocated to the Python process, and is not able to be accessed by anything else. It contains elements such as Object-specific Memory, the Python Core Non-object memory, Object Memory, and Internal Buffers. *This Private Heap is not exclusively RAM!* Rather, Heap memory includes both disc and RAM, and can be dynamically swapped as needed in operations of a program.

The Python Memory Manager communicates with Object-Specific Memory Allocators, which may "instruct" the Python Memory Manager about the peculiarities of an Object Type (specifically type, value, and reference count). This facilitates the Python Memory Manager in assuring these Objects are able to operate within the bounds of the Private Heap. As everything in Python is an object, these rules are highly influential to memory management in Python.

A programmer *can* allocate and release memory blocks within the C library allocator for individual purposes but should never attempt to operate on Python Objects with the functions of the C library allocator. Because the C library allocator and the Python Memory Manager operate on different heaps and implement different algorithms, such attempts are likely to have a fatal result.

There are few circumstances when it's recommended to allocate memory from the Python Heap due to its control by the Python Memory Manager:

-   When the interpreter is extended with new object types written in C
-   When needing to *inform* the Python Memory Manager about the memory needs of an extension module

There are environment variables that can configure the Memory Allocators used by Python.

### Memory Domains

There are three primary domains of Memory Allocator Functions, which are subservient to the General Allocator. While there is no hard requirement to use memory returned by that domain only for the suggested purposes of that domain, it is recommended practice.

#### Raw Domain:

Allocates memory for general-purpose memory buffers. Memory is requested directly to the system. It ensures there is enough memory for all the data of the Python process.

Used when:

-   Allocation *must* go where the Memory Allocator can operate without the GIL

-   Allocating space for the Private Heap if additional memory is needed

#### "Mem" Domain:

Allocates memory for Python buffers and general-purpose memory buffers. This memory is taken from the Private Heap.

Used when:

-   Allocation *must* go where the Memory Allocation operates with the GIL held

#### Object Domain:

Allocates memory for Python Objects. This memory is taken from the Private Heap.

Used when:

-   Allocating space for small objects (512 bytes or less)

-   Allocation of space for large objects(\>512 bytes) also calls the *Raw Allocator* to ensure enough space in the Private Heap

##### Object-Specific Allocators

Object-specific allocators dictate memory management for specific data types. Integers, floats, strings, and lists have Object-Specific Allocators built into Python. It may interact with the Object Domain or the Raw Allocator if needed.

### Memory Allocation & Release

#### Allocating/Claiming Memory

Memory Allocation occurs in *Arenas* and *Pools.*

*Arenas* are the blocks of memory reserved into the Private Heap in allocation by the *Raw Memory Allocator*. They are 256 KB in size. Even the smallest program will take at least 256 KB of memory to function. The most heavily allocated Arena is always used first, if possible, to increase efficiency. Similarly *Python releases memory at the level of Arenas.*

*Pools* are subsections of Arenas that are 4 KB in size. This gives up to 64 Pools per Arena. The division aims to help organize and efficiently use memory. Each pool consists of blocks of only one size class. Pools are only designated within Arenas when necessary (no other Pool with a block of the requested size class is available). *Used* Pools have blocks available, *Full* Pools do not, and *Empty* pools have no size class yet associated with them.

*Blocks* are subsections of Pools that are the smallest unit of memory that can be allocated to an object. An individual object can only be allocated to one block, so objects have either one block or no blocks at all. Blocks are sized in multiples of 8 bytes, and range from 8 to 512 bytes. Block allocation sizes round up to the next multiple of 8 bytes, by necessity.

#### Garbage Collection

Garbage collection is a process of memory optimization: when the program no longer needs allocated memory, it is to be released.

##### By Reference Count

When the *Reference Count* of an Object is zero, Python seeks to remove it from memory. This may trigger a cascade of deletions. For example:

```{python}
c = 10
x = [c,20]
print(x)
```

The variable `x` references a list object `[c,20]`. This list object contains a reference to the integer `10` through the variable `c`. In essence, there are two objects: a list object, and an integer object.

If we deleted `x`'s reference to the list object, the list object would have a Reference Count of 0. The list object `[c,20]` would be slated for deletion. Once deleted, the Reference Count for the integer object `10` would also become 0, and so it would also be deleted.

Garbage Collection by Reference Count is the main garbage collection algorithm of Python, and cannot be disabled. It also does not work if there are cyclic references.

##### Cyclic References

Since Garbage Collection by Reference Count does not work with cyclic references, Python has a Generational Garbage Collection Algorithm it uses instead.

Since cyclic references are only possible with container references, such as tuples and lists, Python scans these objects to determine if they are only referenced cyclicly (and thus eligible for deletion). Tuples only containing immutable types are ignored.

This process is time consuming, and only triggered periodically. The objects that survive each cycle are advanced in generation. Each object type has a threshold of generations, beyond which it is not scanned again.

#### Mutable and Immutable Objects

##### Mutable Objects

Python's built in mutable objects include lists, dictionaries, and sets.

Mutable objects are modified in place within the same area of reference space. However, this does depend on the method that is used to modify the mutable object.

A good example of this is lists. If you modify the list with addition, you call the `__add__` magic method, which creates a new object.

```{python}
test = [1,2,3]
print(f"Test's Original ID: {id(test)}")
test = test + [4]
print(f"Test's New ID: {id(test)}")
print(test)
```

This is in contrast to the `append()` function of list objects, which modifies the list in place.

```{python}
test = [1,2,3]
print(f"Test's Original ID: {id(test)}")
test.append(4)
print(f"Test's New ID: {id(test)}")
print(test)
```

This extends to the original example of two variables which referenced the same mutable object. With `__add__`, the original first variable changes its reference to the new object, while the second "alias" variable continues to reference the original object.

```{python}
test = [1,2,3]
print(f"Test's Original ID: {id(test)}")
alias = test
print(f"Alias's Original ID: {id(alias)}")
test = test + [4]
print(f"Test's New ID: {id(test)}")
print(test)
print(f"Alias's New ID: {id(alias)}")
print(alias)
```

However, using `append()`, which modifies in place, changes the value referenced by both variables at once!

```{python}
test = [1,2,3]
print(f"Test's Original ID: {id(test)}")
alias = test
print(f"Alias's Original ID: {id(alias)}")
test.append(4)
print(f"Test's New ID: {id(test)}")
print(test)
print(f"Alias's New ID: {id(alias)}")
print(alias)
```

##### Immutable Objects

Python's built in immutable objects are tuples, floats, integers, booleans, and strings.

Immutable objects cannot be modified, and thus a new object is created. The main exceptions to this are front-loaded objects in Python, such as the integers from -5 to 256.

```{python}
a = 200
b = 200
print(a==b)
print(a is b)
```

Outside of this front loading, each immutable data type is created new in memory, even if they are equal.

```{python}
a = 4000
b = 4000
print(a==b)
print(a is b)
```

As such, any modification of an immutable object *by necessity* requires enough available memory to have two copies of the object at the same time, so that the variable reference can be shifted from the old object to the new object.

### Multithreading and Multiprocessing in Python

The Python GIL theoretically locks Python into single-threading, and this single-processing. However, mechanisms to enable multithreading and multiprocessing are built into CPython. Many packages and libraries make use of these mechanisms to facilitate efficiencies.

Each Python thread keeps its own *Register* and *Stack*, but can share any *Code* or *Heap* on a single processor. Multiprocessing, by its definition, requires a unique process for each processor, and thus requires full duplication of the entire *execution state*. The libraries and packages in Python thus have to address the *race condition* inherent to multithreading and multiprocessing that are absent in single-thread serial execution that is the default of Python.

Due to the flexibility inherent in Python's *Heap* structure, which can dynamically buffer objects between RAM and disc memory space, the management of large data for processing can be handled with multiple methods such as chunking and lazy evaluations.

### Data Frames in Python

Data Frames are not a part of base Python. However, there are multiple packages that facilitate the use of Data Frames with Python.

#### Pandas

#### Polars

#### Modin

#### Dask

#### Vaex

## R

### A Note on Variables and References

R's variables, like Python, act as References to areas of "memory space" rather than directly storing information themselves. R's variables are called vectors.

```{r}
library(lobstr)
example_list <- list(1,2,3)
example_2 <- example_list
print(identical(example_list,example_2))
print(identical(obj_addr(example_list),obj_addr(example_2)))
```

R's vectors are almost exclusively immutable objects. This means that R uses a behind-the-scenes copy-in-place when modifying objects.

```{r}
example_list <- append(example_list,4)
print(example_list)
print("Spacer for Clarity")
print(example_2)
print(identical(example_list,example_2))
print(identical(obj_addr(example_list),obj_addr(example_2)))
```

While this means that you don't have to worry about mutable data types like you do in Python, it also means that memory demands may increase more dramatically. Even when vectors are "modified," R typically uses a copy-on-update system behind the scenes. It does attempt to perform a modify-in-place modification whenever it can, but it can be difficult to predict if copy-on-update or modify-in-place will occur [@advr2, "Vectors"].

R does not have any 0-dimensional scalars; rather, these objects are stored in one-dimensional vectors with a length of 1 [@advR, "Vectors"].

### General Principles

R, like Python, is an interpreted language. The R interpreter is a program written primarily in C (with some R and Fortran), which takes the R code and interprets it based on the rules of R.

R has a variable-sized work-space, but also works on a static in-memory model. As such, all memory for an R program must be stored in the RAM. While this provides for high computational efficiency and speed, the size of memory allocation is a much greater concern in R than it is in Python due to the lack of dynamic allocation between RAM and disc memory. Many OS's place hard limits on the amount of memory R can allocate for itself. In a 64-bit Windows system there is an OS limit of 4 Gb (0.5 GB) for a 32-bit build of R, and 8 Tb (1 TB) for a 64-bit build of R [@r_manual, "Memory Limits"].

The memory limits on R can be bypassed with certain implementations and packages that call `malloc` directly, which allocates memory in addition to the allocation for the R work-space. However, it should be ensured that this memory is freed on error, user interrupt, or exit [@r_writing_extensions, "Memory Allocation"]

#### Memory Domains

##### Heap Cells

Heap cells of memory are where all vectors, processing, and actual "products" of R are stored. They are parceled into 8-byte *Vcells.* Vcells are considered to be "variable."Default values for the R compiler request 6Mb (0.75 MB) of Vcells on start-up [@r_manual, "Memory Available for Data Storage"]

##### Cons Cells

Cons cells (also called *Ncells)* of memory primarily handle the R language itself, and any objects. Each cell occupies 28 bytes on a 32-bit build of R, and 56 bytes on a 64-bit build of R. This memory also stores the "administrative overhead" of vector objects, such as type and length metadata, called *nodes*. These nodes may be highly recursive in that they point at other nodes and/or values within *Vcells*. Cons cells are considered "fixed." Default values for the R compiler request 350,000 *Ncells* on start-up (9.8 MB on a 32-bit build of R, 19.6 MB on a 64-bit build of R) [@r_manual, "Memory Available for Data Storage"]

### Memory Allocation

#### Allocating/Claiming Memory

R's Memory Allocators are written primarily in C. Vectors of up to 128 bytes are allocated places in memory by R, from the existing held data. Vectors larger than 128 bytes result in a calling the C function `malloc` to allocate additional memory to the heap. All memory allocation methods are described by the R Core team as "relatively expensive." There is the option of the faster C `alloca` method to call additional memory to the stack, but it is listed as "fragile" and causes "danger of overflowing the C stack" [@r_internals, "Allocation Classes"].

#### Garbage Collection

R's Garbage Collector runs based on a "lazy" execution model: it only runs when R is running out of space in its currently allocated memory, and it tries to "clean up" existing memory before requesting additional memory allocation from the OS. However, it will not trigger to reduce memory below the memory allocation requested by R in start-up [@r_manual,"Memory"]

R's Garbage Collector is a *tracing* Garbage Collector: it searches every object and environment reachable from the stack, and the objects reachable from those objects. Anything that the Garbage Collector could not mark as "in use" is marked as eligible for deletion, and released [@r_internals].

The Garbage Collector can be called manually, using the `gc()` command. This is primarily useful after a large object has been removed, or to receive a report on memory usage [@r_manual, "Garbage Collector"].

```{r}
gc()
```

### Multithreading and Multiprocessing

R was built for single-threaded operations with an optimized pipeline. There are a few packages which enable multiprocessing, but they should only be used when the computational load can be reasonably performed in parallel, and the memory burden is not an issue.

### Data Frames in R

Data Frames are the integral, core data structure of R. Data Frames functionally are matrix-like structures formed from a list of coupled vectors, all of which share the same length. Each of these vectors represents a single variable. While each vector should hold items of the same type (i.e. atomic vectors), no two vectors have to share the same data type. [@r_manual, "Data Frames"].

Because Data Frames are lists of vectors, with each vector being a single column, there are significant impacts of R's copy-on-modify behavior. If a column is modified, only that column has to be modified (through copy-on-modify). If a row is modified, *every* column of the data frame has to be modified (through copy-on-modify) [@advr2, "Names and Values"].

*Tibbles* are the primary alternative to Data Frames in R, available as part of the `tidyverse`. Tibbles are simultaneously more relaxed than the base Data Frames, and more opinionated. Since Tibbles share much of the same structure as Data Frames, they operate under the same memory considerations [@tibble_rpkg].
