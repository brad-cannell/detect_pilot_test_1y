---
title: "Importing APS Data For Analysis"
date: "Created: 2019-01-17 <br> Updated: `r Sys.Date()`"
output: 
  html_notebook:
    toc: true
    toc_float: true
    css: custom-css.css
---

# Overview

In this file we read-in the raw data we received from APS on 2018-09-26 through Kiteworks.


# Load packages

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = NA)
Sys.setenv(TZ = "US/Central")
```

```{r message=FALSE}
library(tidyverse)
library(bfuncs)
```

[top](#top)










# Read-in data

```{r}
aps <- readxl::read_excel(
  path = "/Volumes/sph_research/Detect/one_year_data/aps_data.xlsx"
)
```

```{r}
about_data(aps) # 18,152 observations and 47 variables
```

[top](#top)










# Deduplicate rows

```{r}
aps %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  count() %>% 
  ungroup() %>% 
  select(`Duplicate Rows` = n) %>% 
  nrow() # 72 groups of duplicates
```

I manually inspected the duplicate rows. They appear to be genuine duplicates. Below I will drop the duplicate rows.

```{r}
aps <- distinct(aps)
```

```{r}
about_data(aps) # 18,080 observations and 47 variables
```

72 duplicate rows were dropped from the data.

[top](#top)










# Save imported data

```{r}
feather::write_feather(aps, "/Volumes/sph_research/Detect/one_year_data/aps_01_import.feather")
```

[top](#top)










# Session information

```{r echo=FALSE}
rm(list = ls())
```

```{r echo=FALSE}
sessionInfo()
```