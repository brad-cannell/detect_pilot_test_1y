---
title: "Importing APS Data For Analysis"
date: "Created: 2019-01-17 <br> Updated: `r Sys.Date()`"
---

# ⭐️Overview

In this file we read-in the raw data we received from APS on 2018-09-26 through Kiteworks.


# 📦Load packages

```{r message=FALSE}
library(dplyr)
```


# 🌐Connect to server

Connect to the UTHealth server.

```{bash eval=FALSE}
# For Brad
open 'smb://islgpcifs.uthouston.edu/sph_research/DETECT/one_year_data/'
```


# 📥Read-in data

* Make sure to connect to the UTHealth server using the path stored in NOTES.

```{r}
aps <- readxl::read_excel("/Volumes/one_year_data/aps_data.xlsx")
```

```{r}
dim(aps) # 18,152    47
```


# Deduplicate rows

```{r}
aps %>% 
  group_by_all() %>% 
  filter(n() > 1) %>% 
  count() %>% 
  ungroup() %>% 
  select(`Duplicate Rows` = n) %>% 
  nrow() # 72 groups of duplicates
```

I manually inspected the duplicate rows. They appear to be genuine duplicates. Below I will drop the duplicate rows.

```{r}
aps <- distinct(aps)
```

```{r}
dim(aps) # 18,080    47
```

18152 - 18080 = 72 duplicate rows were dropped from the data.


# Save APS data

As an RDS file.

```{r}
readr::write_rds(aps, "/Volumes/one_year_data/aps_01_import.rds")
```

# Session information

```{r echo=FALSE}
rm(list = ls())
```

```{r echo=FALSE}
sessionInfo()
```

